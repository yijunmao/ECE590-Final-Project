{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final C: Visual servoing control\n",
    "\n",
    "For your final, you will be implementing a visual servoing controller with several variants, and testing its behavior under different sensor and object movement conditions.  The robot is given an eye-in-hand camera, and an object pose detector is assumed given to you.\n",
    "\n",
    "A basic implementation of the HW4 control scheme is given to you.  In this scheme, the error signal is only the  translation of the object.  The link keeps its orientation fixed regardless of the object orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b75fcd55f44859b53c10d1d36e6876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KlamptWidget(scene={u'object': {u'matrix': [1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1], u'uuid': u'67c4eâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#these might be useful\n",
    "from klampt import *\n",
    "from klampt.math import so3,se3,vectorops\n",
    "from IPython.display import display,clear_output\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "import ipyklampt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "world = WorldModel()\n",
    "fn = \"data/ur5Cart.xml\"\n",
    "res = world.readFile(fn)\n",
    "robot = world.robot(0)\n",
    "\n",
    "#load the object\n",
    "objfn = \"data/objects/cylinder.obj\"\n",
    "#use this if you want to test how well your controller handles object rotation about z...\n",
    "#objfn = \"data/objects/block_small.obj\"\n",
    "res = world.readFile(objfn)\n",
    "obj = world.rigidObject(0)\n",
    "Tobj_orig = (so3.identity(),[0.75,0.21,1.2])\n",
    "#FOR TESTING: start out out of the camera's FOV\n",
    "#Tobj_orig = (so3.identity(),[0.75,-0.4,1.2])\n",
    "obj.setTransform(*Tobj_orig)\n",
    "\n",
    "#set the home configuration\n",
    "qhome  = robot.getConfig()\n",
    "qhome[1] = 2.3562\n",
    "qhome[2] = -math.pi/3.0\n",
    "qhome[3] = math.pi*2/3.0\n",
    "qhome[4] = -0.9\n",
    "qhome[5] = math.pi/2.0\n",
    "robot.setConfig(qhome)\n",
    "\n",
    "kvis = ipyklampt.KlamptWidget(world)\n",
    "display(kvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cameraLink = robot.link(7)\n",
    "cameraXform = ([0,-1,0,0,0,-1,1,0,0],[0.05,0,-0.03])\n",
    "\n",
    "kvis.add_xform(\"camera\",length=0.1)\n",
    "kvis.set_transform(\"camera\",*se3.mul(cameraLink.getTransform(),cameraXform))\n",
    "\n",
    "def setConfig(q):\n",
    "    robot.setConfig(q)\n",
    "    kvis.set_transform(\"camera\",*se3.mul(cameraLink.getTransform(),cameraXform))\n",
    "    kvis.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emulated object pose: ([-1.7754899901929982e-09, 0.14666656540331252, 0.9891859878671934, 0.9999999999999998, 0.0, 1.7949000612323396e-09, 2.632518272231427e-10, 0.9891859878671937, -0.14666656540331258], [0.10084999950729188, -0.014243196761691612, 0.22450448243478027])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sensorNoiseRotation = 0.0\n",
    "sensorNoiseTranslation = 0.0\n",
    "#FOR TESTING: add noise to the camera estimate\n",
    "#sensorNoiseRotation = 0.1\n",
    "#sensorNoiseTranslation = 0.02\n",
    "sensorFOV = math.radians(90.0)\n",
    "sensorW = 640\n",
    "sensorH = 480\n",
    "\n",
    "def emulateSensor():\n",
    "    \"\"\"The sensor either produces None if the object is not seen, or otherwise a rigid transform.\"\"\"\n",
    "    camWorldXform = se3.mul(cameraLink.getTransform(),cameraXform)\n",
    "    objXform = obj.getTransform()\n",
    "    camXform = se3.mul(se3.inv(camWorldXform),objXform)\n",
    "    x,y,z = camXform[1]\n",
    "    slope = math.tan(sensorFOV*0.5)\n",
    "    if z < 0.1:\n",
    "        #minimum distance\n",
    "        return None\n",
    "    if abs(x) > z*slope:\n",
    "        #out of horizontal FOV\n",
    "        return None\n",
    "    if abs(y)*sensorW/sensorH > z*slope:\n",
    "        #out of vertical FOV\n",
    "        return None\n",
    "    \n",
    "    eR = [random.gauss(0,sensorNoiseRotation*z) for i in range(3)]\n",
    "    et = [random.gauss(0,sensorNoiseTranslation*z) for i in range(3)]\n",
    "    return se3.mul(camXform,(so3.from_rotation_vector(eR),et))\n",
    "\n",
    "print \"Emulated object pose:\",emulateSensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: your visual servoing code here\n",
    "\n",
    "#try to keep the object 30cm away from the sensor, in the center of the image\n",
    "desiredObjectCameraShift = [0,0,0.3]\n",
    "\n",
    "#you can put other state variables here\n",
    "numLoops = 0\n",
    "\n",
    "def visualServoInit(robot,q):\n",
    "    \"\"\"Initialize the controller.  This is called at the start of each simulation.\"\"\"\n",
    "    global numLoops\n",
    "    #place whatever code you want here\n",
    "    numLoops = 0\n",
    "\n",
    "def visualServoFixedOrientation(robot,dt,q,sensorReading):\n",
    "    \"\"\"Takes the current sensed q and sensor reading, and outputs a desired position command for the robot.\n",
    "    \n",
    "    This is run every dt seconds.\n",
    "    \n",
    "    Do not \"cheat\" and use the object position directly!\n",
    "    \"\"\"\n",
    "    global numLoops\n",
    "    \n",
    "    numLoops += 1\n",
    "    if sensorReading is None:\n",
    "        #didn't see the object, do nothing\n",
    "        return q\n",
    "    #provided code from HW4\n",
    "    T = vectorops.mul(vectorops.sub(desiredObjectCameraShift,sensorReading[1]),0.1)\n",
    "    camWorldXform = se3.mul(cameraLink.getTransform(),cameraXform)\n",
    "    #convert the error into link local coordinates\n",
    "    eR = [0,0,0]   #rotation error\n",
    "    et = so3.apply(camWorldXform[0],T)\n",
    "    robot.setConfig(q)\n",
    "    J = cameraLink.getJacobian((0,0,0))\n",
    "    #jacobian has orientation rows on top, translation rows on bottom\n",
    "    #several joints are not allowed to move, so zero those out\n",
    "    qmin,qmax = robot.getJointLimits()\n",
    "    for j in range(cameraLink.index+1):\n",
    "        if qmin[j] == qmax[j]:\n",
    "            for i in range(6):\n",
    "                J[i][j] = 0.0\n",
    "    err = list(eR) + list(et)\n",
    "    #pseudoinverse method\n",
    "    dq = np.dot(np.linalg.pinv(J),err)\n",
    "    return vectorops.madd(q,dq,-1.0)\n",
    "\n",
    "def visualServoFreeOrientation(robot,dt,q,sensorReading):\n",
    "    \"\"\"Takes the current sensed q and sensor reading, and outputs a desired position command for the robot.\n",
    "    \n",
    "    This is run every dt seconds.\n",
    "    \n",
    "    Do not \"cheat\" and use the object position directly!\n",
    "    \"\"\"\n",
    "    global numLoops\n",
    "    \n",
    "    numLoops += 1\n",
    "    if sensorReading is None:\n",
    "        #didn't see the object, do nothing\n",
    "        return q\n",
    "    #do nothing? TODO\n",
    "    return q\n",
    "\n",
    "def visualServoMatchOrientation(robot,dt,q,sensorReading):\n",
    "    \"\"\"Takes the current sensed q and sensor reading, and outputs a desired position command for the robot.\n",
    "    \n",
    "    This is run every dt seconds.\n",
    "    \n",
    "    Do not \"cheat\" and use the object position directly!\n",
    "    \"\"\"\n",
    "    global numLoops\n",
    "    \n",
    "    numLoops += 1\n",
    "    if sensorReading is None:\n",
    "        #didn't see the object, do nothing\n",
    "        return q\n",
    "    #do nothing? TODO\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object movement models\n",
    "\n",
    "There are several object movement models available for you to test.  When you are developing and testing your methods, use the options below to examine how your controller behaves in response to certain object behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectMovement(t):\n",
    "    \"\"\"Returns w,v where w is the object's angular velocity and v is the translational velocity\"\"\"\n",
    "    #FOR TESTING: change the object behavior here\n",
    "    #stationary\n",
    "    return [0,0,0],[0,0,0]\n",
    "    \n",
    "    #step change at t=2\n",
    "    #if t >= 1.95 and t < 2.05:\n",
    "    #    return [0,0,0],[0,-1,0]\n",
    "    #else:\n",
    "    #    return [0,0,0],[0,0,0]\n",
    "    \n",
    "    #moving back and forth\n",
    "    #return [0,0,0],[math.cos(t*0.5)*0.02,0,0]\n",
    "    \n",
    "    #moving back and forth quickly\n",
    "    #return [0,0,0],[math.cos(t*2)*0.08,0,0]\n",
    "    \n",
    "    #moving side to side with a moderate amplitude\n",
    "    #return [0,0,0],[0,math.cos(t*2)*0.3,0]\n",
    "    \n",
    "    #moving side to side with a large amplitude\n",
    "    #return [0,0,0],[0,math.cos(t*2)*0.6,0]\n",
    "    \n",
    "    #rotating in place\n",
    "    #return [0,math.cos(t)*0.2,0],[0,0,0]\n",
    "    \n",
    "    #A different rotation angle\n",
    "    #return [math.cos(t)*0.2,0,0],[0,0,0]\n",
    "    \n",
    "    #moving into view (use this when starting at the out-of-FOV location\n",
    "    #return [0,math.cos(t)*0.2,0],[0,math.exp(-t*2),0]\n",
    "    \n",
    "    #moving in and out of view \n",
    "    #w = [0,math.cos(t)*0.2,0]\n",
    "    #u = t % 4\n",
    "    #if u >= 1.95 and u <= 2.05:\n",
    "    #    return [5,w[1],w[2]],[0,10,0]\n",
    "    #elif u >= 2.45 and u <= 2.55:\n",
    "    #    return w,[0,-10,0]\n",
    "    #return w,[0,0,0]\n",
    "    \n",
    "    #moving noisily\n",
    "    #rvel = 0.02\n",
    "    #tvel = 0.05\n",
    "    #return [random.gauss(0,rvel) for i in range(3)],[random.gauss(0,tvel) for i in range(3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scroll up to look at the visualization... starting in 2s\n",
      "Starting simulation now\n",
      "Stopping simulation\n"
     ]
    }
   ],
   "source": [
    "#test your code by running this cell\n",
    "obj.setTransform(*Tobj_orig)\n",
    "setConfig(qhome)\n",
    "#initialize the controller\n",
    "visualServoInit(robot,qhome)\n",
    "\n",
    "print \"Scroll up to look at the visualization... starting in 2s\"\n",
    "time.sleep(2.0)  #this gives you time to scroll up to watch the movement\n",
    "print \"Starting simulation now\"\n",
    "dt = 0.1 \n",
    "t = 0\n",
    "for i in xrange(100):  \n",
    "    #simulate the sensor and controller\n",
    "    s = emulateSensor()\n",
    "    q = robot.getConfig()\n",
    "    \n",
    "    #FOR TESTING: choose your controller here\n",
    "    qnext = visualServoFixedOrientation(robot,dt,q,s)\n",
    "    #qnext = visualServoFreeOrientation(robot,dt,q,s)\n",
    "    #qnext = visualServoMatchOrientation(robot,dt,q,s)\n",
    "    \n",
    "    #simulate the object movement\n",
    "    T = obj.getTransform()\n",
    "    w,v = objectMovement(t)\n",
    "    rot = so3.mul(so3.from_rotation_vector(vectorops.mul(w,dt)),T[0])\n",
    "    pos = vectorops.madd(T[1],v,dt)\n",
    "    obj.setTransform(rot,pos)\n",
    "    \n",
    "    dq = vectorops.div(vectorops.sub(qnext,q),dt)\n",
    "    vmax = robot.getVelocityLimits()\n",
    "    for i in range(len(vmax)):\n",
    "        if abs(dq[i]) > vmax[i]+1e-10:\n",
    "            print \"VELOCITY LIMIT\",i,\"VIOLATED\",dq[i],\">\",vmax[i]\n",
    "    setConfig(qnext)\n",
    "    t += dt\n",
    "    kvis.update()\n",
    "    time.sleep(0.1)\n",
    "print \"Stopping simulation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Your goal is to implement and study control strategies for the following:\n",
    "\n",
    "1. Modify the provided controller to estimate the the velocity of the object, and incorporate this into the visualServoFixedOrientation control loop so that the robot matches the object's velocity better. \n",
    "\n",
    "2. For large magnitude movements of the object, the fixed orientation constraint is a big limitation.  Implement visualServoFreeOrientation so that the camera link is not required to stay at the same orientation.  \n",
    "\n",
    "3. In visualServoMatchOrientation implement a controller that, when it first sees an object, remembers the object's relative orientation.  Thereafter, it matches the camera's orientation to maintain the same orientation, so if the object tilts, the camera should tilt to match.  (If the object disappears and reappears, it should match the object's new orientation)\n",
    "\n",
    "In each of these implementations, the robot's joint positions and velocities should not exceed their limits, and the jerkiness of the robot should not be severe (visually obvious).  Don't worry about self-collision or collision with the table.\n",
    "\n",
    "In your report, precisely and in technical English language (not code, although pseudocode may be acceptable), describe your strategy for implementing each controller, including but not limited to:\n",
    "\n",
    "* The mathematical formulation of each control strategy\n",
    "\n",
    "* The mathematical formulation of the object velocity estimation strategy\n",
    "\n",
    "* A mathematical formulation of how you handle null-space motions with the free-orientation controller.\n",
    "\n",
    "* A state diagram of the orientation-matching controller\n",
    "\n",
    "* Your choice of gain constants and a rationale for choosing these (e.g., did you perform empirical tuning, use intuition, or is there a theoretical justification?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unit testing\n",
    "\n",
    "Perform unit testing of your controller, and describe the performance metrics and test procedures that you are using.  Interpret the observations you have made, including but not limited to:\n",
    "\n",
    "* Whether performance is sensitive to certain parameters.\n",
    "* Whether performance limitations or bottlenecks can be identified.\n",
    "* Whether unit tests agree with theoretical behavior.\n",
    "\n",
    "Parts of the provided code marked FOR_TESTING should be modified and tuned to provide these results. Also, you might find it useful to develop auxiliary testing code that measures performance and generates data for different parameters.\n",
    "\n",
    "Specifically, you should at least:\n",
    "\n",
    "* Test Controller 1 with object motions of different velocities.  Test how it responds to step functions and noise in the object movement. Test how it responds to noise to the sensor measurements (see the sensor emulation cell above).\n",
    "\n",
    "* Test Controller 1 and 2 with large magnitude movements, and examine how well the controllers are able to follow the object movement.\n",
    "\n",
    "* Test Controller 3 with varying object orientations, including the case in which the object appears into the field of view, or moves in and out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
